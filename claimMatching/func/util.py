"""
Created on Wed Jul 29 16:40:36 2020
This file contains several utility functions used by the claim matcher.
@author: brocklin
"""
import numpy as np
from nltk.corpus import stopwords

def get_unique_words(documents):
    """
    Helper function that returns every unique word present in a set of documents.

    :param documents: list of string documents containing words
    :return: list of all unique lowercase string words across the documents
    """
    unique_words = []
    for document in documents:
        doc_words = document.split(' ')
        unique_words.extend(doc_words)
    unique_words = [word.lower() for word in unique_words]
    return list(dict.fromkeys(unique_words))

def build_word_encoder_decoder(unique_words):
    """
    Given a set of unique words, creates an encoder that maps from words to integers
    and a decoder that maps from integers back to words

    :param unique_words: list of string words to encode and decode into ints
    :return: a dict to map each word in unique_words to an int and a list to decode each int to its word
    """
    word_decoder = []
    word_encoder = {}
    index = 0
    for word in unique_words:
        word_decoder.append(word.lower())
        word_encoder[word.lower()] = index
        index += 1
    return word_encoder, word_decoder

def build_word_count_matrix(documents, unique_words, word_encoder):
    """
    Given a set of D documents and W unique words, creates a DxW matrix
    to store the number of occurrences of each word across all documents.

    :param documents: all documents to use for counting words
    :param unique_words: all unique words that occur within the documents
    :param word_encoder: a dictionary mapping each unique word to an integer
    :return: a matrix storing the number of times each word appears in each document
    """
    word_counts = np.zeros((len(documents), len(unique_words)), dtype=np.int32)
    for index, claim in enumerate(documents):
        for word in claim.split(' '):
            word_counts[index, word_encoder[word.lower()]] += 1
    return word_counts

def build_idf_matrix(documents, word_counts):
    """
    Creates the IDF matrix for TF-IDF matrix computation.

    :param documents: all documents
    :param word_counts: the counts of all words across documents
    :return: the computed IDF matrix
    """
    word_counts[word_counts > 0] = 1
    return np.log(len(documents) / np.sum(word_counts, axis=0))

def do_tf_idf(documents):
    """
    Performs vectorized tf-idf and returns a list of all keywords to filter tweet data with.

    :param documents: list of all documents to perform tf-idf over
    :return: list of most important keywords from claims to filter tweets with
    """
    # get all unique words
    unique_words = get_unique_words(documents)
    word_encoder, word_decoder = build_word_encoder_decoder(unique_words)
    word_counts = build_word_count_matrix(documents, unique_words, word_encoder)

    tf_matrix = word_counts / np.sum(word_counts, axis=1).reshape(-1, 1)
    idf_matrix = build_idf_matrix(documents, word_counts.copy())
    tf_idf_matrix = tf_matrix * idf_matrix

    return tf_idf_matrix, word_encoder, word_decoder

def get_filtering_words(documents, cfg):
    """
    Uses TF-IDF to find words to use for filtering the search set.

    :param documents: documents from the target set to extract filter words from
    :param cfg: configuration dictionary
    :return: a list of words to filter search set documents with
    """
    tf_idf_matrix, word_encoder, word_decoder = do_tf_idf(documents)
    tf_idf_averages = np.sum(tf_idf_matrix, axis=0)
    filter_exclude = stopwords.words('english')
    filter_exclude.extend(cfg['stopwords'])
    for word in filter_exclude:
        if word in word_decoder:
            tf_idf_averages[word_encoder[word]] = 0

    top_overall = np.argsort(tf_idf_averages)[-cfg['num_filters']:]
    filter_words = []
    for term in top_overall:
        filter_words.append(word_decoder[term])
    return filter_words

def make_list_unique(elem_list):
    """
    Removes all duplicated values from the supplied list.

    :param elem_list: list to remove duplicate elements from
    :return: new list with all unique elements of the original list
    """
    return list(dict.fromkeys(elem_list))

def filter_documents(documents, filter_words):
    """
    Given a set of documents and keywords to filter documents with, filters
    out all documents that do not contain at least one of the keywords.

    :param documents: list of documents
    :param filter_words: list of keywords generated by tf-idf over the claims
    :return: a filtered list of the tweets and a list of the matching keywords for each tweet
    """
    all_matches = []
    new_docs = []
    for document in documents:
        keyword_matches = []
        contains_filter = False
        for word in document.split(' '):
            if word.lower() in filter_words:
                contains_filter = True
                keyword_matches.append(word.lower())
        if contains_filter:
            keyword_matches = make_list_unique(keyword_matches)
            new_docs.append(document)
            all_matches.append(keyword_matches)
    return new_docs, all_matches
